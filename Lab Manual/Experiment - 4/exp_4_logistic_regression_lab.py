# -*- coding: utf-8 -*-
"""Exp-4 Logistic Regression Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C8EhNR3trwu7UlnAXnuzJs8AQDHGKHnu

To understand the logistic Regression that includes non-linearity to linear regression
"""

# 1.Load the basic libraries and packages\

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

# 2.	Load the dataset

data = pd.read_csv("https://raw.githubusercontent.com/nishithkotak/machine-learning/master/diabetes.csv")
data

# 3.	Analyze the dataset

data.describe()

# 4.	Normalize the data

def Feature_Normalization(X):
  X = (X - np.mean(X , axis = 0)) / np.std(X , axis = 0)
  return X , np.mean(X , axis = 0) ,  np.std(X , axis = 0)

# 5.	Pre-process the data

x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42)

# 6.	Visualize the Data

for feature in data.columns :
    sns.distplot(data[feature])
    plt.show()

# 7.Separate the feature and prediction value columns

x = data.iloc[: , :-1].values
y = data.iloc[: , -1].values

def Sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 8.	Write the Hypothesis Function

def Hypothesis(theta, X):
    return Sigmoid(np.dot(X, theta))

# 9. Write the Cost Function

def Cost_function(theta, X, y):

    m = len(y)  # number of training examples
    h = Sigmoid(np.dot(X, theta))  # hypothesis (predicted probabilities)


    cost = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))

    return cost

# 10.	Write the Gradient Descent optimization algorithm

def Gradient_Descent(X, y, theta_array, alpha, epochs):
    m = len(y)
    cost_history = []

    for i in range(epochs):
        h = Sigmoid(np.dot(X, theta_array))
        gradient = (1/m) * np.dot(X.T, (h - y))
        theta_array = theta_array - alpha * gradient
        cost = Cost_function(theta_array, X, y)
        cost_history.append(cost)

    return theta_array, cost_history

# 11.	Apply Feature Normalization technique over the data

x_train , train_mean , train_std = Feature_Normalization(x_train)
x_test , test_mean , test_std = Feature_Normalization(x_test)

# 12.	Apply the training over the dataset to minimize the loss

def Training(X, y, alpha, epochs):
  theta_array = [0] * (x_train.shape[1])
  m = len(x)
  cost_history = []

  for i in range(epochs):
    theta_array , cost_history = Gradient_Descent(X , y , theta_array , alpha , epochs)

  return theta_array , cost_history

# Apply the Logistics Regression

alpha = 0.001
epochs = 100
theta_array , cost_history = Training(x_train  , y_train , alpha , epochs)

# 13.	Observe the cost function vs iterations learning curve

x = np.arange(0, epochs)
plt.plot(x, cost_history)
plt.xlabel('Epochs')
plt.ylabel('Cost')
plt.show()